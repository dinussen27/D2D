{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Importing libaries </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from No_sync.credentials import config \n",
    "import requests\n",
    "from pprint import pprint\n",
    "#from authentication import get_token\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Handling API requests and data extraction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token():\n",
    "  \"\"\"\n",
    "  Get an access token from the API\n",
    "  \"\"\"\n",
    "\n",
    "  if not config['client_id']:\n",
    "    raise ValueError('client_id must be set in credentials.py')\n",
    "\n",
    "  if not config['client_secret']:\n",
    "    raise ValueError('client_secret must be set in credentials.py')\n",
    "\n",
    "  req = requests.post(config['token_url'],\n",
    "    data={\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': config['client_id'],\n",
    "        'client_secret': config['client_secret'],\n",
    "        'scope': 'api'\n",
    "    },\n",
    "    headers={'content-type': 'application/x-www-form-urlencoded'})\n",
    "\n",
    "  req.raise_for_status()\n",
    "  print('Token request successful')\n",
    "  return req.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_week_summary(token, year, week):\n",
    "  \"\"\"\n",
    "  Get the weekly summary for a given year and week\n",
    "  \"\"\"\n",
    "  url = f\"{config['api_base_url']}/v1/geodata/fishhealth/locality/{year}/{week}\"\n",
    "  headers ={\n",
    "    'authorization': 'Bearer ' + token['access_token'],\n",
    "    'content-type': 'application/json',\n",
    "  }\n",
    "\n",
    "  response = requests.get(url, headers=headers)\n",
    "  response.raise_for_status()\n",
    "  return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample_request(token):\n",
    "  \"\"\"\n",
    "  Make a sample request to the API\n",
    "  \"\"\"\n",
    "  url = f\"{config['api_base_url']}/v1/sample/auth\"\n",
    "  headers ={\n",
    "    'authorization': 'Bearer ' + token['access_token'],\n",
    "    'content-type': 'application/json',\n",
    "  }\n",
    "\n",
    "  response = requests.get(url, headers=headers)\n",
    "  response.raise_for_status()\n",
    "  return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting token from https://id.barentswatch.no/connect/token, using client_id dinussen.sivarasalingam@nmbu.no:Fishdata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token request successful\n"
     ]
    }
   ],
   "source": [
    "#request a token\n",
    "print(f\"Requesting token from {config['token_url']}, using client_id {config['client_id']}.\")\n",
    "token = get_token()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request to the api was successful - authentication worked.\n",
      "{'id': 1879182644,\n",
      " 'name': 'https://www.barentswatch.no',\n",
      " 'updated': '2023-10-02T13:48:27.0597974+02:00'}\n"
     ]
    }
   ],
   "source": [
    "# test token\n",
    "response = make_sample_request(token)\n",
    "print('Request to the api was successful - authentication worked.')\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data transformation </h2>\n",
    "<h3> Getting year worth of data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def weeks_of_the_year():\n",
    "    \"\"\"\n",
    "    Create a DataFrame with the week number and year for each week of the year\n",
    "    \"\"\"\n",
    "    # Get the current year and the week of the year\n",
    "    today = datetime.now()\n",
    "    year = today.year\n",
    "    day_of_year = today.strftime('%j')\n",
    "    week_of_year = (int(day_of_year) - 1) // 7 + 1\n",
    "    \n",
    "    # Calculate the number of weeks left in the previous year\n",
    "    weeks_left_last_year = 52 - week_of_year\n",
    "    \n",
    "    # Create DataFrames for the current year and the remaining weeks from the previous year\n",
    "    week_df = pd.DataFrame({'Week': range(1, week_of_year + 1), 'Year': [year] * week_of_year})\n",
    "    week_df_last = pd.DataFrame({'Week': range(1, weeks_left_last_year + 1), 'Year': [year - 1] * weeks_left_last_year})\n",
    "\n",
    "    # Concatenate the two DataFrames\n",
    "    week_df = pd.concat([week_df_last, week_df], ignore_index=True)\n",
    "    # add week_of_year to week where year is current year - 1\n",
    "    week_df.loc[week_df['Year'] == year - 1, 'Week'] += week_of_year\n",
    "\n",
    "    return week_df\n",
    "\n",
    "# Example usage:\n",
    "weeks_df = weeks_of_the_year()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(weeksummary):\n",
    "    \"\"\"\n",
    "    Create a DataFrame from the week summary\n",
    "    \"\"\"\n",
    "    df_data = pd.DataFrame()\n",
    "    #print(weeksummary)\n",
    "    for element in weeksummary:\n",
    "        #print(element)\n",
    "        if element == 'localities':\n",
    "            lines = []\n",
    "            for information in weeksummary['localities']:\n",
    "                line = []\n",
    "                for key, value in information.items():\n",
    "                    #print(key, value)\n",
    "                    df_data[key] = value\n",
    "                    line.append(value)\n",
    "                df_data['year'] = (weeksummary['year'])\n",
    "                line.append(weeksummary['year'])\n",
    "                df_data['week'] = weeksummary['week']\n",
    "                line.append(weeksummary['week'])\n",
    "                #print(line)\n",
    "            \n",
    "                #add line to df_data as row\n",
    "                lines.append(line)\n",
    "    \n",
    "    df_data = pd.DataFrame(lines, columns = df_data.columns)\n",
    "    #print(df_data)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_data(weeks_df):\n",
    "    \"\"\"\n",
    "    This function returns a dataframe with all the data from the weeks in weeks_df\n",
    "    \"\"\"\n",
    "    teller = 0\n",
    "    df_data = pd.DataFrame()\n",
    "    for week, year in weeks_df.values:\n",
    "        teller +=1\n",
    "        #print(week, year)\n",
    "        weeksummary= get_week_summary(token,year,week)\n",
    "        #print(weeksummary)\n",
    "        df_to_concat = make_df(weeksummary)\n",
    "        #print(df_to_concat)\n",
    "        frames = [df_data, df_to_concat]\n",
    "        df_data = pd.concat(frames, ignore_index=True)\n",
    "        #print(teller)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n",
      "year\n",
      "week\n",
      "localities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(89782, 21)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_oneyear = get_year_data(weeks_df)\n",
    "summary_oneyear.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_oneyear.to_csv('Data/summary_oneyear.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find null values for each column\n",
    "# summary_oneyear.isnull().sum()\n",
    "\n",
    "# #remove avgAdultFemaleLice column\n",
    "# summary_oneyear = summary_oneyear.drop(columns=['avgAdultFemaleLice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Get lice count from one locality </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detailed_week(token, year, week, localityId):\n",
    "  \"\"\"\n",
    "  Get the weekly summary for a given year and week\n",
    "  \"\"\"\n",
    "  url = f\"{config['api_base_url']}/v1/geodata/fishhealth/locality/{localityId}/{year}/{week}\"\n",
    "  headers ={\n",
    "    'authorization': 'Bearer ' + token['access_token'],\n",
    "    'content-type': 'application/json',\n",
    "  }\n",
    "\n",
    "  response = requests.get(url, headers=headers)\n",
    "  response.raise_for_status()\n",
    "  return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns_from_json(json_data, desired_columns=['avgAdultFemaleLice', 'avgMobileLice', 'avgStationaryLice', 'localityName', 'speciesList']):\n",
    "    \"\"\"\n",
    "    Extract specified columns from a JSON input and create a DataFrame.\n",
    "\n",
    "    :param json_data: JSON data as a dictionary.\n",
    "    :param desired_columns: List of column names to extract.\n",
    "    :return: A DataFrame containing the extracted columns.\n",
    "    \"\"\"\n",
    "    # Extract columns from 'localityWeek' if they exist\n",
    "    locality_week_data = json_data.get('localityWeek', {})\n",
    "    locality_week_columns = {col: locality_week_data.get(col) for col in ['avgAdultFemaleLice', 'avgMobileLice', 'avgStationaryLice']}\n",
    "    \n",
    "    # Extract 'speciesList' from 'aquaCultureRegister' if it exists\n",
    "    aqua_culture_register_data = json_data.get('aquaCultureRegister', {})\n",
    "    species_list = aqua_culture_register_data.get('speciesList')\n",
    "\n",
    "    # Combine the extracted columns into a single dictionary\n",
    "    filtered_data = {\n",
    "        'avgAdultFemaleLice': locality_week_columns.get('avgAdultFemaleLice'),\n",
    "        'avgMobileLice': locality_week_columns.get('avgMobileLice'),\n",
    "        'avgStationaryLice': locality_week_columns.get('avgStationaryLice'),\n",
    "        'localityName': json_data.get('localityName'),\n",
    "        'speciesList': species_list\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary into a DataFrame\n",
    "    df = pd.DataFrame([filtered_data])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_lice_counts(token, weeks_df, locality=32277):\n",
    "\n",
    "    lice_counts = pd.DataFrame()\n",
    "    for week, year in weeks_df.values:\n",
    "        weekdetails = get_detailed_week(token, year, week, locality)\n",
    "\n",
    "        # Extract the desired columns from the JSON data\n",
    "        df = extract_columns_from_json(weekdetails)\n",
    "\n",
    "        # Add the year and week columns\n",
    "        df['year'] = year\n",
    "        df['week'] = week\n",
    "\n",
    "        # Concatenate the DataFrames\n",
    "        lice_counts = pd.concat([lice_counts, df], ignore_index=True)\n",
    "    return lice_counts\n",
    "lice_counts = get_lice_counts(token, weeks_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "lice_counts.to_csv('Data/lice_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avgAdultFemaleLice</th>\n",
       "      <th>avgMobileLice</th>\n",
       "      <th>avgStationaryLice</th>\n",
       "      <th>localityName</th>\n",
       "      <th>speciesList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Nmbu Fiskelaboratoriet</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  avgAdultFemaleLice avgMobileLice avgStationaryLice            localityName  \\\n",
       "0               None          None              None  Nmbu Fiskelaboratoriet   \n",
       "\n",
       "  speciesList  \n",
       "0        None  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Connecting to Cassandra cluster and data loading</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Cassandra\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x2376c2c7250>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a keyspace\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS fish_keyspace WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x2376691f410>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new table (first time only)\n",
    "session.set_keyspace('fish_keyspace')\n",
    "\n",
    "# Drop the table if it already exists\n",
    "session.execute(\"DROP TABLE IF EXISTS fish_keyspace.fish_table_year;\") \n",
    "\n",
    "# Create a new table\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fish_table_year (\n",
    "        localityNo INT,\n",
    "        localityweekid INT PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        hasReportedLice BOOLEAN,\n",
    "        isFallow BOOLEAN,\n",
    "        avgAdultFemaleLice DOUBLE,\n",
    "        hasCleanerfishDeployed BOOLEAN,\n",
    "        hasMechanicalRemoval BOOLEAN,\n",
    "        hasSubstanceTreatments BOOLEAN,\n",
    "        hasPd BOOLEAN,\n",
    "        hasIla BOOLEAN,\n",
    "        municipalityNo INT,\n",
    "        municipality TEXT,\n",
    "        lat DOUBLE,\n",
    "        lon DOUBLE,\n",
    "        isOnLand BOOLEAN,\n",
    "        inFilteredSelection BOOLEAN,\n",
    "        hasSalmonoids BOOLEAN,\n",
    "        isSlaughterHoldingCage BOOLEAN,\n",
    "        year INT,\n",
    "        week INT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# session.execute(\"CREATE TABLE IF NOT EXISTS fish_table_year (localityNo INT, localityWeekId INT, year INT, week INT, PRIMARY KEY ((localityNo, localityWeekId, year, week)), name TEXT, hasReportedLice BOOLEAN, isFallow BOOLEAN, avgAdultFemaleLice DOUBLE, hasCleanerfishDeployed BOOLEAN, hasMechanicalRemoval BOOLEAN, hasSubstanceTreatments BOOLEAN, hasPd BOOLEAN, hasIla BOOLEAN, municipalityNo INT, municipality TEXT, lat DOUBLE, lon DOUBLE, isOnLand BOOLEAN, inFilteredSelection BOOLEAN, hasSalmonoids BOOLEAN, isSlaughterHoldingCage BOOLEAN);\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk-20\"\n",
    "# If you are using environments in Python, you can set the environment variables like this:\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "#os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "# Set the Hadoop version to the one you are using, e.g., none:\n",
    "#os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"without\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').getOrCreate()\n",
    "# Some warnings are to be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- localityNo: integer (nullable = true)\n",
      " |-- localityWeekId: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- hasReportedLice: boolean (nullable = true)\n",
      " |-- isFallow: boolean (nullable = true)\n",
      " |-- avgAdultFemaleLice: double (nullable = true)\n",
      " |-- hasCleanerfishDeployed: boolean (nullable = true)\n",
      " |-- hasMechanicalRemoval: boolean (nullable = true)\n",
      " |-- hasSubstanceTreatments: boolean (nullable = true)\n",
      " |-- hasPd: boolean (nullable = true)\n",
      " |-- hasIla: boolean (nullable = true)\n",
      " |-- municipalityNo: integer (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- isOnLand: boolean (nullable = true)\n",
      " |-- inFilteredSelection: boolean (nullable = true)\n",
      " |-- hasSalmonoids: boolean (nullable = true)\n",
      " |-- isSlaughterHoldingCage: boolean (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YearData = spark.read.csv('Data/summary_oneyear.csv', header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "#show types\n",
    "YearData.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Attempting to write to C* Table but missing\nprimary key columns: [localityweekid]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dinus\\OneDrive - Norwegian University of Life Sciences\\NMBU\\2023H\\IND320\\Oblig\\D2D\\DataSources.ipynb Cell 27\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dinus/OneDrive%20-%20Norwegian%20University%20of%20Life%20Sciences/NMBU/2023H/IND320/Oblig/D2D/DataSources.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m keyspace \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfish_keyspace\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dinus/OneDrive%20-%20Norwegian%20University%20of%20Life%20Sciences/NMBU/2023H/IND320/Oblig/D2D/DataSources.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m table \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfish_table_year\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dinus/OneDrive%20-%20Norwegian%20University%20of%20Life%20Sciences/NMBU/2023H/IND320/Oblig/D2D/DataSources.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m YearData\u001b[39m.\u001b[39mwrite \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dinus/OneDrive%20-%20Norwegian%20University%20of%20Life%20Sciences/NMBU/2023H/IND320/Oblig/D2D/DataSources.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39morg.apache.spark.sql.cassandra\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dinus/OneDrive%20-%20Norwegian%20University%20of%20Life%20Sciences/NMBU/2023H/IND320/Oblig/D2D/DataSources.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m, table) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dinus/OneDrive%20-%20Norwegian%20University%20of%20Life%20Sciences/NMBU/2023H/IND320/Oblig/D2D/DataSources.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mkeyspace\u001b[39m\u001b[39m\"\u001b[39m, keyspace) \\\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dinus/OneDrive%20-%20Norwegian%20University%20of%20Life%20Sciences/NMBU/2023H/IND320/Oblig/D2D/DataSources.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m.\u001b[39mmode(\u001b[39m\"\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msave()\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1396\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m)\n\u001b[0;32m   1395\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1396\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[0;32m   1397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1398\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave(path)\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: Attempting to write to C* Table but missing\nprimary key columns: [localityweekid]"
     ]
    }
   ],
   "source": [
    "keyspace = \"fish_keyspace\"\n",
    "table = \"fish_table_year\"\n",
    "\n",
    "YearData.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"table\", table) \\\n",
    "    .option(\"keyspace\", keyspace) \\\n",
    "    .mode(\"append\").save()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IND320",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
